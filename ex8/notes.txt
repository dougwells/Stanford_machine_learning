Stochastic = randomly selected
  Run modified gradient descent on each datapoint.  In series.  Take small steps
  Each step might get closer or further
  Generally 1-10 times thru data gets you close to global optima/minima
  Each step requires 1 calculation

Batch Gradient Descent
  Calculate Gradient Descent for entire training set.  In series but BIG steps
  Each step should get closer to global optima/minima (if alpha is set sufficiently small)
  For a dataset of 300M, computationally expensive.  Each step requires 300M calcs

Map Reduce
  Divide 400M dataset into 4 batches of 100M.  Have each 100M dataset analyzed by 1 computer
  4 computers.  Each analyze 100M sub-training set.
  Simply add up the gradients & divide by 400M to find new Thetas
